[
  {
    "id": "eval-1-transformer-attention",
    "prompt": "Transformerの自己注意機構（Self-Attention）について、PyTorchでの実装例を含むブログ記事を書いてください。\n\nタイトル: Transformerの自己注意機構を実装から理解する\n概要: 自己注意機構の仕組みを直感的に説明し、PyTorchでゼロから実装する方法を解説する\nキーワード: Transformer, 自己注意機構, PyTorch, 深層学習, NLP",
    "expectations": [
      "記事が3000-5000字の範囲内である",
      "PyTorchのコード例が少なくとも2つ含まれている",
      "コードに適切なコメントが付いている",
      "自己注意機構の概念が具体例や比喩を使って説明されている",
      "数式がインライン `\\\\(` とブロック `$$` で正しく記述されている",
      "絵文字が使用されていない",
      "導入、技術解説、実装、まとめの構成になっている",
      "Markdownファイルとして出力されている"
    ]
  },
  {
    "id": "eval-2-batch-normalization",
    "prompt": "バッチ正規化（Batch Normalization）の仕組みと実装について、初心者にも分かりやすいブログ記事を書いてください。\n\nキーワード: バッチ正規化, PyTorch, ニューラルネットワーク, 学習安定化",
    "expectations": [
      "記事が3000-5000字の範囲内である",
      "バッチ正規化の概念が身近な例えで説明されている",
      "PyTorchでの実装例が含まれている",
      "よくある問題やエラーへの対処法が含まれている",
      "コードが実際に動作する形で提供されている",
      "専門用語に適切な説明が付いている"
    ]
  },
  {
    "id": "eval-3-resnet-implementation",
    "prompt": "ResNet（残差ネットワーク）をPyTorchで実装する方法について、ブログ記事を書いてください。特に、残差接続がなぜ重要なのかを詳しく説明してください。",
    "expectations": [
      "記事が3000-5000字の範囲内である",
      "残差接続の概念と重要性が明確に説明されている",
      "ResNetブロックの実装例が含まれている",
      "実践的なユースケースや応用例が含まれている",
      "パフォーマンス最適化のヒントが含まれている",
      "フレンドリーで親しみやすいトーンである"
    ]
  }
]
